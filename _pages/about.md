---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Wei Han is now a research scientist. His research interests now focus on a broad range of topics regarding multimodal large language models (LLMs), mainly focus on the following topics with specific research questions:
- Efficient Data-centric Alignment of LLMs
  - **What makes reasoning data “high-quality”?**
  - **Can we develop automatic methods to curate or distill alignment data?**
  - **How can we balance correctness and efficiency in reasoning output?**
- Large Reasoning Models
  - **How to controll the Depth and Style of Reasoning?**
  - **How to preserve reasoning consistency and faithfulness and avoid reasoning path that are logically flawed or disconnected from the final answer?**
  - **Test-Time Control over Reasoning Behavior**
- Agentic LLMs
  - **Long-Horizon planning and decomposition**
  - **Memory and context management**
  - **Tool Use and Decision Making**

Other interested directions or those I have worked on include *retrive-augmented generation (RAG), long-context modeling, video understanding, multimodal representation learning and semi-supervised learning (SSL)*. For more information, you can check my CV [here](https://Clement25.github.io/files/CV.pdf).


Collaborations are always welcome! If you are looking forward to research experience or trying academic projects with me, feel free to drop me an email (better to attach your CV or personal website so that I can get to know you well). 

<span style="color: red;"> I am actively seeking research scientist/engineer positions starting in 2025 based in Singapore/China/U.S (no particular preference). Feel free to reach out to me if you are interested in my profile and my skill set meets the position requirements! </span>

## News
🔥🔥🔥  <span style="color:red"> **[2025.01]** </span> Our work on multimodal recommendation has been accepeted to **NAACL 2025**.
👉 💻 [code](https://github.com/declare-lab/Sealing) &ensp; 📖 [paper](https://arxiv.org/pdf/2505.01255)


🔥🔥🔥 <span style="color:red"> **[2024.10]** </span> **We release the latest work on *long-context modeling*, in which we propose a new architecture for efficient post-training and finetuning.**
👉 💻 [code](https://github.com/Clement25/SharedLLM) &ensp; 📖 [paper](https://arxiv.org/pdf/2410.19318)


[2024.03] Our work on efficient sampling in video question answering has been accepeted to **NAACL 2024**.
👉 💻 [code](https://github.com/declare-lab/Sealing) &ensp; 📖 [paper](https://arxiv.org/pdf/2307.04192.pdf)

[2024.02] We have released the code of **Auto Scaling**, the first framework for automatic instruction scaling in (multimodal) instruction fine-tuning.
👉 💻 [code](https://github.com/declare-lab/Auto-Scaling) &ensp; 📖 [paper](https://arxiv.org/pdf/2402.14492.pdf)

[2022.10] Two papers are accpeted to EMNLP and Findings of EMNLP 2022!
